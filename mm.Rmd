```{r}
library(tidyr)
library(dplyr)
library(voivod) # devtools::install_github("forecastingresearch/voi-vod")
library(ggplot2)
library(xpt)

library(ggplot2)
library(scales)
```

```{r}
geoMeanOfOddsCalc <- function(x) {
  if (0 %in% x) {
    x[x == 0] <- 1e-14
  }
  odds <- x / (1 - x)
  geoMeanOfOdds <- exp(mean(log(odds)))
  # Convert back to probability
  geoMeanOfOdds <- geoMeanOfOdds / (geoMeanOfOdds + 1)
  return(geoMeanOfOdds)
}
```

```{r}
# Get median PU for each camp
medianPU <- pSheet_final %>%
  group_by(Camp) %>%
  summarize(medianPU = median(PU, na.rm = TRUE))

# CASE 1: C determines U

pu_c <- medianPU$medianPU[medianPU$Camp == "Concerned"]
puc_c <- 1 - 1e-15
pc_c <- pu_c

pu_s <- medianPU$medianPU[medianPU$Camp == "Skeptical"]
puc_s <- 1 - 1e-15
pc_s <- pu_s

VoI_log_s <- VoI_log(pu_s, puc_s, pc_s)
VoI_log_c <- VoI_log(pu_c, puc_c, pc_c)

# (check)
punotc_s <- punotc(pc_s, puc_s, pu_s)
punotc_c <- punotc(pc_c, puc_c, pu_c)

VoD_log_gmod_max <- VoD_log_gmod(pu_s, pu_c, puc_s, puc_c, pc_s, pc_c, punotc(pc_s, puc_s, pu_s), punotc(pc_c, puc_c, pu_c))

# CASE 2: ~C determines U

puc_c <- 1e-15
pc_c <- 1 - pu_c

puc_s <- 1e-15
pc_s <- 1 - pu_s

VoI_log_c <- VoI_log(pu_c, puc_c, pc_c)
VoI_log_s <- VoI_log(pu_s, puc_s, pc_s)

# (check)
punotc_s <- punotc(pc_s, puc_s, pu_s)
punotc_c <- punotc(pc_c, puc_c, pu_c)

VoD_log_gmod_max <- VoD_log_gmod(pu_s, pu_c, puc_s, puc_c, pc_s, pc_c, punotc(pc_s, puc_s, pu_s), punotc(pc_c, puc_c, pu_c))
```

```{r}
VoD_log_gmod_easy <- function(dt, id1, id2) {
  pu_a <- dt[dt$person == id1, "PU"] %>% pull()
  pu_b <- dt[dt$person == id2, "PU"] %>% pull()
  puc_a <- dt[dt$person == id1, "PUc"] %>% pull()
  puc_b <- dt[dt$person == id2, "PUc"] %>% pull()
  pc_a <- dt[dt$person == id1, "Pc"] %>% pull()
  pc_b <- dt[dt$person == id2, "Pc"] %>% pull()
  punotc_a <- dt[dt$person == id1, "punotc"] %>% pull()
  punotc_b <- dt[dt$person == id2, "punotc"] %>% pull()

  answer <- VoD_log_gmod(pu_a, pu_b, puc_a, puc_b, pc_a, pc_b, punotc_a, punotc_b)
  return(answer)
}

maxVOD_easy <- function(dt, id1, id2) {
  pu_a <- dt[dt$person == id1, "PU"] %>% pull()
  pu_b <- dt[dt$person == id2, "PU"] %>% pull()
  answer <- maxVOD(pu_a, pu_b)
  return(answer)
}

VoD_level_easy <- function(dt, id1, id2) {
  pu_a <- dt[dt$person == id1, "PU"] %>% pull()
  pu_b <- dt[dt$person == id2, "PU"] %>% pull()
  puc_a <- dt[dt$person == id1, "PUc"] %>% pull()
  puc_b <- dt[dt$person == id2, "PUc"] %>% pull()
  pc_a <- dt[dt$person == id1, "Pc"] %>% pull()
  pc_b <- dt[dt$person == id2, "Pc"] %>% pull()
  punotc_a <- dt[dt$person == id1, "punotc"] %>% pull()
  punotc_b <- dt[dt$person == id2, "punotc"] %>% pull()

  answer <- VoD_naive(pu_a, pu_b, puc_a, puc_b, pc_a, pc_b, punotc_a, punotc_b)
  return(answer)
}

maxVOD_level_easy <- function(dt, id1, id2) {
  pu_a <- dt[dt$person == id1, "PU"] %>% pull()
  pu_b <- dt[dt$person == id2, "PU"] %>% pull()
  answer <- maxVOD(pu_a, pu_b, fun = function(a, b) abs(a - b))
  return(answer)
}
```

```{r}
reshapedData <- read.csv("FORECASTS.csv")
```

```{r}
# Add Log VOI (KL Divergence) now if it's P8 data
reshapedData <- reshapedData %>%
  rowwise() %>%
  mutate(punotc = punotc(Pc, PUc, PU))
```

```{r}
# For rows with incoherent forecasts, make PU NA (we'll drop these)
reshapedData <- reshapedData %>%
  mutate(PU = ifelse(PUc * Pc > PU, NA, PU))
```

```{r}
reshapedData <- reshapedData %>%
  mutate(VoI_log = VoI_log(PU, PUc, Pc, punotc)) %>%
  mutate(VoI_naive = VoI_naive(PU, PUc, Pc, punotc)) %>%
  mutate(person = paste0(person, " (", Camp, ")"))
```

```{r}
reshapedData <- reshapedData %>%
  filter(!is.na(PU)) %>% # cases where P(c) * P(U|c) > P(U) OR either P(c) or P(U|c) were NA
  filter(!is.nan(VoI_log))
```

```{r}
# Luke policies --> Muehlhauser policies
reshapedData <- reshapedData %>%
  mutate(ID = ifelse(ID == "Luke policies", "Muehlhauser policies", ID))
```

```{r}
bresults <- reshapedData %>%
  group_by(Camp, ID) %>%
  summarize(
    median = quantile50(VoI_log),
    x = list(boot_results(VoI_log, statistic = "quantile50"))
  ) %>%
  unnest(x)
```

```{r}
palette1 <- c("#59C3C3", "#52489C")
palette2 <- c("#006E90", "#F18F01")
```

```{r}
PC <- reshapedData %>%
  group_by(ID, Camp) %>%
  summarize(median = quantile50(Pc)) %>%
  # Pivot wider: Camp
  pivot_wider(names_from = Camp, values_from = median)

write.csv(PC, "PC.csv", row.names = FALSE)
```

```{r}
# P(c) by question
p_by_question_graph(reshapedData, "Camp", "P(Question Resolves \"Yes\")", "Pc", "ID", palette2, "p8_graphs/pc_by_question.png")

# P(U|c) by question
p_by_question_graph(reshapedData, "Camp", "P(AI Extinction by 2100 | Question Resolves \"Yes\")", "PUc", "ID", palette2, "p8_graphs/puc_by_question.png")
```

```{r}
# 2030 lookup table
lookup <- reshapedData %>%
  group_by(ID, Camp) %>%
  summarize(
    happens = quantile50(PUc),
    doesnt = quantile50(punotc)
  ) %>%
  arrange(ID) %>%
  # Melt to wide format
  pivot_wider(names_from = Camp, values_from = c(happens, doesnt))

write.csv(lookup, "lookup.csv", row.names = FALSE)
```

```{r}
# Add % of max VOI
reshapedData <- reshapedData %>%
  rowwise() %>%
  mutate(max_VoI = VoI_log(PU, 1 - (1E-16), PU)) %>%
  mutate(percent_max_VoI = VoI_log / max_VoI)

# How many times is VoI_log bigger than max_VoI?
sum(reshapedData$VoI_log > reshapedData$max_VoI)
```

```{r}
# IRR
library(dplyr)
library(tidyr)
library(irr)

kripp_alpha_result <- list()

for (grp in c("Skeptical", "Concerned")) {
  # Reshape the data to wide format
  df_wide <- reshapedData %>%
    filter(Camp == grp) %>%
    select(ID, person, percent_max_VoI, Camp) %>% # Selecting item, rater and the rating columns
    pivot_wider(names_from = ID, values_from = percent_max_VoI, id_cols = person) %>% # Pivoting the data to wide format
    mutate(across(all_of(unique(reshapedData$ID)), as.double, .names = "{col}")) # Converting list-columns to double

  # Converting the data frame to a matrix (excluding the first column)
  mat_wide <- as.matrix(df_wide[-1])

  # Compute Krippendorff's Alpha
  kripp_alpha_result[[grp]] <- irr::kripp.alpha(mat_wide, method = "interval") # excluding the first column which might be item names/IDs
}
```

```{r}
# Calculate pearson and spearman correlations between every possible pair of
# forecasters across all questions

my_corrs <- reshapedData %>%
  pivot_wider(names_from = person, values_from = percent_max_VoI, id_cols = ID) %>%
  as.data.frame()

# Set ID as row names using base R
rownames(my_corrs) <- my_corrs$ID

# Remove the ID column
my_corrs <- my_corrs[, !(names(my_corrs) %in% c("ID"))]

# Function to calculate correlations
calculate_correlations <- function(df) {
  combn(names(df), 2, function(pair) {
    person1 <- df[[pair[1]]]
    person2 <- df[[pair[2]]]

    list(
      pair = paste(pair, collapse = " - "),
      raw_corr = cor(person1, person2, method = "pearson", use = "complete.obs"),
      spearman_corr = cor(person1, person2, method = "spearman", use = "complete.obs")
    )
  }, simplify = FALSE) %>% bind_rows()
}
```

```{r}
# Calculate correlations for each pair
my_corrs <- calculate_correlations(my_corrs)

# Filter to only pairs with TWO SKEPTICAL
skeptical_corrs <- my_corrs %>%
  filter(grepl("Skeptical", pair)) %>%
  filter(!grepl("Concerned", pair))

# Summary statistics
summary(skeptical_corrs$raw_corr)
summary(skeptical_corrs$spearman_corr)

# Filter to only pairs with TWO CONCERNED
concerned_corrs <- my_corrs %>%
  filter(grepl("Concerned", pair)) %>%
  filter(!grepl("Skeptical", pair))

# Summary statistics
summary(concerned_corrs$raw_corr)
summary(concerned_corrs$spearman_corr)

# Filter to only pairs with ONE SKEPTICAL AND ONE CONCERNED
mixed_corrs <- my_corrs %>%
  filter(grepl("Concerned", pair)) %>%
  filter(grepl("Skeptical", pair))

# Summary statistics
summary(mixed_corrs$raw_corr)
summary(mixed_corrs$spearman_corr)
```

```{r}
# Calculate the 50% quantiles for each dataset
mixed_medians <- mixed_corrs %>%
  summarize(
    raw_corr = quantile50(raw_corr),
    spearman_corr = quantile50(spearman_corr)
  )

skeptical_medians <- skeptical_corrs %>%
  summarize(
    raw_corr = quantile50(raw_corr),
    spearman_corr = quantile50(spearman_corr)
  )

concerned_medians <- concerned_corrs %>%
  summarize(
    raw_corr = quantile50(raw_corr),
    spearman_corr = quantile50(spearman_corr)
  )

# Combine the data into one dataframe
combined_medians <- rbind(
  cbind(mixed_medians, group = "mixed"),
  cbind(skeptical_medians, group = "skeptical"),
  cbind(concerned_medians, group = "concerned")
)
```

```{r}
# Calculate the 95% CI for each correlation type and name (mixed, skeptical, concerned)
ci_results <- lapply(numeric_results, function(x) {
  # Calculate the 95% CI
  ci <- quantile(x, c(0.025, .25, .50, .75, 0.975))

  # Return the CI
  return(ci)
})
```

```{r}
# Simple corrs plots
library(dplyr)
library(ggplot2)

# Add a category label to each data frame
skeptical_corrs$category <- "Skeptical"
concerned_corrs$category <- "Concerned"
mixed_corrs$category <- "Cross-Camp"

# Combine into a single data frame
all_corrs <- bind_rows(skeptical_corrs, concerned_corrs, mixed_corrs)
```

```{r}
# Boxplot of Pearson correlations for all categories
ggplot(all_corrs, aes(x = category, y = raw_corr, fill = category)) +
  geom_boxplot() +
  labs(
    title = "Boxplot of Pearson Correlations",
    x = "Within- or Between-Camps",
    y = "Pearson Correlation"
  ) +
  theme_minimal() +
  scale_fill_manual(values = c("Skeptical" = "#7575d8", "Concerned" = "#dd6565", "Cross-Camp" = "yellow"))

ggsave("pearson_corrs.png", width = 9, height = 8)
```

```{r}
# Boxplot of Spearman correlations for all categories
ggplot(all_corrs, aes(x = category, y = spearman_corr, fill = category)) +
  geom_boxplot() +
  labs(
    title = "Boxplot of Spearman Correlations",
    x = "Within- or Between-Camps",
    y = "Spearman Correlation"
  ) +
  theme_minimal() +
  scale_fill_manual(values = c("Skeptical" = "#7575d8", "Concerned" = "#dd6565", "Cross-Camp" = "yellow"))

ggsave("spearman_corrs.png", width = 9, height = 8)
```

```{r}
juice <- reshapedData %>%
  group_by(ID, Camp) %>%
  summarize(
    medianVOI = quantile50(VoI_log),
    medianPercentMaxVOI = quantile50(percent_max_VoI)
  ) %>%
  pivot_wider(names_from = Camp, values_from = c(medianVOI, medianPercentMaxVOI)) %>%
  select(ID, medianVOI_Concerned, medianPercentMaxVOI_Concerned, medianVOI_Skeptical, medianPercentMaxVOI_Skeptical)

write.csv(juice, "VOI_medians.csv", row.names = FALSE)
```

```{r}
juice_naive <- reshapedData %>%
  group_by(ID, Camp) %>%
  summarize(
    medianVOI = quantile50(VoI_naive),
    medianPercentMaxVOI = quantile50(VoI_naive / VoI_naive(PU, 1 - (1E-16), PU))
  ) %>% # Divide by max naive VOI
  pivot_wider(names_from = Camp, values_from = c(medianVOI, medianPercentMaxVOI)) %>%
  select(ID, medianVOI_Concerned, medianPercentMaxVOI_Concerned, medianVOI_Skeptical, medianPercentMaxVOI_Skeptical)

write.csv(juice_naive, "VOI_medians_naive.csv", row.names = FALSE)
```

```{r}
# Quick plot dual axes VoI and % of max VoI
qr <- reshapedData %>%
  group_by(ID, Camp) %>%
  summarize(VoI_log = quantile50(VoI_log), percent_max_VoI = quantile50(percent_max_VoI))

p <- ggplot(qr, aes(x = VoI_log, y = percent_max_VoI)) +
  geom_point(aes(color = Camp), alpha = 0.7) + # The color aesthetic is optional, and you can remove it if not needed
  labs(x = "Log VOI", y = "% of Max Log VOI") +
  theme_minimal()

ggsave("VoI_log_vs_percent_max_VoI.png", p, width = 8, height = 8)

write.csv(qr, "abc.csv", row.names = FALSE)
```

```{r}
# Bootstrapping but with consistent bootstraps across questions

library(dplyr)
library(boot)
library(purrr)

# Simulate some data
set.seed(123)

# Pre-generate bootstrap indices
R <- 1000
boot_indices <- lapply(1:R, function(i) sample(1:11, 11, replace = TRUE))
```

```{r}
# Modify boot_results to use pre-generated bootstrap indices
boot_results_modified <- function(data, indices, statistic, colname = "VoI_log") {
  stat_fun <- match.fun(statistic)

  resampled_data <- data[indices, , drop = FALSE]
  stat_val <- stat_fun(resampled_data[[colname]])

  return(stat_val)
}

if (file.exists("results_VoI.csv")) {
  results <- read.csv("results_VoI.csv", header = TRUE)
  # If "Luke policies" is in results$ID, replace with "Muehlhauser policies"
  if ("Luke policies" %in% results$ID) {
    results$ID[results$ID == "Luke policies"] <- "Muehlhauser policies"
  }
} else {
  # Apply bootstrapping using pre-generated indices
  results <- reshapedData %>%
    group_by(Camp, ID) %>%
    summarize(
      medians = list(
        map(boot_indices, ~ boot_results_modified(cur_data(), .x, "quantile50"))
      ),
    ) %>%
    unnest(medians)

  results$medians <- sapply(results$medians, `[[`, "50%")

  write.csv(results, "results_VoI.csv", row.names = FALSE)
}
```

```{r}
boot_results_modified_VoD <- function(data, indices, statistic) {
  stat_fun <- match.fun(statistic)

  # Separate data into two camps
  data_s <- data[data$Camp == "Skeptical", ]
  data_c <- data[data$Camp == "Concerned", ]

  resampled_data_s <- data_s[indices, , drop = FALSE]
  resampled_data_c <- data_c[indices, , drop = FALSE]

  # Give "people" unique IDs 1 through 22
  resampled_data_s$person <- c(1:11)
  resampled_data_c$person <- c(12:22)

  dt <- rbind(resampled_data_s, resampled_data_c)
  VoDs <- c()

  # Get cross-camp VoDs for each pair of people
  for (i in 1:11) {
    for (j in 11:22) {
      pu_a <- dt[dt$person == i, "PU"] %>% pull()
      pu_b <- dt[dt$person == j, "PU"] %>% pull()
      puc_a <- dt[dt$person == i, "PUc"] %>% pull()
      puc_b <- dt[dt$person == j, "PUc"] %>% pull()
      pc_a <- dt[dt$person == i, "Pc"] %>% pull()
      pc_b <- dt[dt$person == j, "Pc"] %>% pull()
      punotc_a <- dt[dt$person == i, "punotc"] %>% pull()
      punotc_b <- dt[dt$person == j, "punotc"] %>% pull()
      answer <- VoD_log_gmod(pu_a, pu_b, puc_a, puc_b, pc_a, pc_b, punotc_a, punotc_b)
      # Append answer to VoDs
      VoDs <- c(VoDs, answer)
    }
  }

  stat_val <- stat_fun(VoDs)

  return(stat_val)
}

if (file.exists("results_VoD.csv")) {
  results_VoD <- read.csv("results_VoD.csv", header = TRUE)
} else {
  results_VoD <- reshapedData %>%
    group_by(ID) %>%
    summarize(
      medians_VoD = list(
        map(boot_indices, ~ boot_results_modified_VoD(
          cur_data(),
          .x,
          "quantile50"
        ))
      )
    ) %>%
    unnest(medians_VoD)

  results_VoD$medians_VoD <- sapply(results_VoD$medians_VoD, `[[`, "50%")

  write.csv(results_VoD, "results_VoD.csv", row.names = FALSE)
}
```

```{r}
# Make the matrix: Two matrix figures, one for each camp, where rows (i) are
# questions and columns (j) are questions and each cell tells us % of the time
# in the bootstrapped worlds that Qi is higher VOI than Qj.

currCamp <- "Skeptical"

# Function to compare medians between two IDs and count occurrences
compare_medians <- function(id1, id2, data) {
  medians_id1 <- data %>%
    filter(ID == id1) %>%
    pull(medians)
  medians_id2 <- data %>%
    filter(ID == id2) %>%
    pull(medians)
  sum(medians_id1 > medians_id2)
}

# Create an empty matrix to store results
unique_IDs <- unique(results$ID)
matrix_size <- length(unique_IDs)
comparison_matrix <- matrix(0,
  nrow = matrix_size, ncol = matrix_size,
  dimnames = list(unique_IDs, unique_IDs)
)

campResults <- results %>% filter(Camp == currCamp)

# Fill in the matrix
for (i in 1:matrix_size) {
  print(paste0(i, " of ", matrix_size))
  for (j in 1:matrix_size) {
    if (i != j) {
      comparison_matrix[i, j] <- compare_medians(unique_IDs[i], unique_IDs[j], campResults)
    }
  }
}

comparison_matrix <- comparison_matrix / R * 100
```

```{r}
# Show me the row of comparison matrix names "Supers changing minds"
comparison_matrix["Supers changing minds", ]

# How many of the numbers in that row are between 74 and 75?
sum(comparison_matrix["Supers changing minds", ] >= 74 &
  comparison_matrix["Supers changing minds", ] <= 75)

comparison_matrix["Alignment researchers changing minds", ]

# What's the range of values in that row?
range(comparison_matrix["Alignment researchers changing minds", ])

# What's the range if you take out zero?
range(comparison_matrix["Alignment researchers changing minds", ] %>% .[. != 0])
```

```{r}
# Comparison matrix for VoD

# Function to compare medians between two IDs and count occurrences
compare_medians <- function(id1, id2, data) {
  medians_id1 <- data %>%
    filter(ID == id1) %>%
    pull(medians_VoD)
  medians_id2 <- data %>%
    filter(ID == id2) %>%
    pull(medians_VoD)
  sum(medians_id1 > medians_id2)
}

# Create an empty matrix to store results
unique_IDs <- unique(results_VoD$ID)
matrix_size <- length(unique_IDs)
comparison_matrix_VoD <- matrix(0,
  nrow = matrix_size, ncol = matrix_size,
  dimnames = list(unique_IDs, unique_IDs)
)

# Fill in the matrix
for (i in 1:matrix_size) {
  for (j in 1:matrix_size) {
    if (i != j) {
      comparison_matrix_VoD[i, j] <- compare_medians(unique_IDs[i], unique_IDs[j], results_VoD)
    }
  }
}

comparison_matrix_VoD <- comparison_matrix_VoD / R * 100 # Convert to percent
```

```{r}
# Make a nice heatmap of that comparison_matrix
library(ggplot2)

# Convert the matrix to a tidy format for ggplot2
comparison_df <- as.data.frame(as.table(comparison_matrix))

# Order by this camp's median
ordering <- bresults %>%
  filter(Camp == currCamp) %>%
  arrange(median) %>%
  pull(ID)

comparison_df$Var1 <- factor(comparison_df$Var1, levels = ordering)
comparison_df$Var2 <- factor(comparison_df$Var2, levels = ordering)

# Remove upper triangle
comparison_df <- comparison_df[as.integer(comparison_df$Var2) <= as.integer(comparison_df$Var1), ]

# Create the heatmap
heatmap_plot <- ggplot(comparison_df, aes(x = Var1, y = Var2, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradientn(colours = topo.colors(10)) + # You can choose your own color gradient
  theme_minimal() +
  labs(
    title = paste0("Comparison Heatmap for ", currCamp, " (R=", R, ")"),
    x = "ID 1",
    y = "ID 2",
    fill = "ID1 VOI > ID2 VOI\n (% of samples)"
  ) +
  coord_fixed(ratio = 1) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.margin = margin(, , , 2, "cm"),
    axis.ticks.y = element_blank()
  ) # Hide y-axis ticks

ggsave(paste0(currCamp, "_VoI_heatmap.png"), heatmap_plot, width = 10, height = 8)
```

```{r}
# NOW VOD: Make a nice heatmap of that comparison_matrix
library(ggplot2)

# Convert the matrix to a tidy format for ggplot2
comparison_df_VoD <- as.data.frame(as.table(comparison_matrix_VoD))

# Order by this median pairwise VoD in "our world"
ordering <- VoD_quantiles %>%
  arrange(median) %>%
  pull(ID)

comparison_df_VoD$Var1 <- factor(comparison_df_VoD$Var1, levels = ordering)
comparison_df_VoD$Var2 <- factor(comparison_df_VoD$Var2, levels = ordering)

# Remove upper triangle
comparison_df_VoD <- comparison_df_VoD[as.integer(comparison_df_VoD$Var2) <= as.integer(comparison_df_VoD$Var1), ]

# Create the heatmap
heatmap_plot <- ggplot(comparison_df_VoD, aes(x = Var1, y = Var2, fill = Freq)) +
  geom_tile() +
  scale_fill_gradientn(colours = topo.colors(10)) + # You can choose your own color gradient
  theme_minimal() +
  labs(
    title = paste0("Comparison Heatmap for VOD (R=", R, ")"),
    x = "ID 1",
    y = "ID 2",
    fill = "ID1 VOD > ID2 VOD\n (% of samples)"
  ) +
  coord_fixed(ratio = 1) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.margin = margin(, , , 2, "cm")
  )

ggsave("VoD_heatmap.png", heatmap_plot, width = 10, height = 8)
```

```{r}
# Add direction of update
reshapedData <- reshapedData %>%
  mutate(direction = ifelse(PUc > PU, "up", ifelse(PUc == PU, "no change", "down")))
```

```{r}
# Rank questions by median VoI three ways: all forecasters; skeptic camp only; concerned camp only
central_tendency <- reshapedData %>%
  group_by(ID) %>%
  summarise(
    q50_KL = quantile(VoI_log, p = 0.5, type = 1, na.rm = TRUE),
    q50_KL_skeptic = quantile(VoI_log[Camp == "Skeptical"], p = 0.5, type = 1, na.rm = TRUE),
    q50_KL_concerned = quantile(VoI_log[Camp == "Concerned"], p = 0.5, type = 1, na.rm = TRUE),
    median_KL = median(VoI_log, na.rm = TRUE),
    median_KL_skeptic = median(VoI_log[Camp == "Skeptical"], na.rm = TRUE),
    median_KL_concerned = median(VoI_log[Camp == "Concerned"], na.rm = TRUE),
    median_naive = median(VoI_naive, na.rm = TRUE),
    median_naive_skeptic = median(VoI_naive[Camp == "Skeptical"], na.rm = TRUE),
    median_naive_concerned = median(VoI_naive[Camp == "Concerned"], na.rm = TRUE)
  )

write.csv(central_tendency, "median_individual_KL_and_naive.csv", row.names = FALSE)
```

```{r}
# Median forecasters - how many times is it zero (just checking)
central_tendency %>% filter(q50_KL_concerned == 0)
```

```{r}
# Who are the median forecasters in each camp for each question? Filter reshapedData to those rows
arj <- merge(central_tendency, reshapedData) %>%
  select(ID, starts_with("q50_KL_"), person, Camp, PU, Pc, PUc, punotc, VoI_log)

# Filter arj to the rows within ID where VoI_log == median_KL_skeptic or median_KL_concerned
arjj <- arj %>%
  group_by(ID) %>%
  filter(VoI_log == q50_KL_skeptic | VoI_log == q50_KL_concerned) %>%
  select(ID, person, Camp, VoI_log, PU, PUc, Pc, punotc) %>%
  filter(VoI_log != 0) %>%
  mutate(VoD_log_gmod = ifelse(n() >= 2, VoD_log_gmod_easy(cur_data(), person[1], person[2]), NA)) %>%
  arrange(VoD_log_gmod, ID)

write.csv(arjj, "median_by_VOI_stats.csv", row.names = FALSE)
```

```{r}
# How many IDs in arjj have two or more people (VoD should be defined for 19 questions, I think?)
arjj %>%
  group_by(ID) %>%
  filter(n() >= 2) %>%
  summarize(n = n())

VoD_between_medians <- arjj %>%
  select(ID, VoD_log_gmod) %>%
  distinct()
```

```{r}
# AVERAGE RANK METHOD: FOR Log VOI
# First, rank questions by person. Then take average rank of each question
df <- reshapedData %>%
  group_by(person) %>%
  mutate(
    rank_KL = rank(-VoI_log, ties.method = "min", na.last = "keep"),
    rank_naive = rank(-VoI_naive, ties.method = "min", na.last = "keep")
  )

df_skeptic <- reshapedData %>%
  filter(Camp == "Skeptical") %>%
  group_by(person) %>%
  mutate(
    rank_skeptic_KL = rank(-VoI_log, ties.method = "min", na.last = "keep"),
    rank_skeptic_naive = rank(-VoI_naive, ties.method = "min", na.last = "keep")
  ) %>%
  select(person, ID, rank_skeptic_KL, rank_skeptic_naive)

df_concerned <- reshapedData %>%
  filter(Camp == "Concerned") %>%
  group_by(person) %>%
  mutate(
    rank_concerned_KL = rank(-VoI_log, ties.method = "min", na.last = "keep"),
    rank_concerned_naive = rank(-VoI_naive, ties.method = "min", na.last = "keep")
  ) %>%
  select(person, ID, rank_concerned_KL, rank_concerned_naive)

df <- merge(df, df_skeptic, by = c("ID", "person"), all.x = TRUE)
df <- merge(df, df_concerned, by = c("ID", "person"), all.x = TRUE)

# Take average rank per question
avg_ranks <- df %>%
  group_by(ID) %>%
  summarize(
    avg_rank_KL = mean(rank_KL, na.rm = TRUE),
    avg_rank_skeptic_KL = mean(rank_skeptic_KL, na.rm = TRUE),
    avg_rank_concerned_KL = mean(rank_concerned_naive, na.rm = TRUE),
    avg_rank_naive = mean(rank_naive, na.rm = TRUE),
    avg_rank_skeptic_naive = mean(rank_skeptic_naive, na.rm = TRUE),
    avg_rank_concerned_naive = mean(rank_concerned_naive, na.rm = TRUE)
  )

write.csv(avg_ranks, "mean_ranks_KL_and_naive.csv", row.names = FALSE)
```

```{r}
# What's the max VoI_log for just the skeptic camp?
max(reshapedData$VoI_log[reshapedData$Camp == "Skeptical"], na.rm = TRUE)

# Order ID levels by median (or mean)
reshapedData$ID <- factor(reshapedData$ID, levels = central_tendency$ID[order(central_tendency$q50_KL)])

# create the plot
plotMe <- function(reshapedData) {
  ggplot(reshapedData, aes(x = ID, y = VoI_log, color = Camp)) +
    geom_point() +
    scale_y_continuous(
      trans = log10_trans(),
      breaks = trans_breaks("log10", function(x) 10^x),
      labels = trans_format("log10", math_format(10^.x))
    ) +
    geom_point(data = central_tendency, aes(x = ID, y = q50_KL_concerned), color = "#0072c4", shape = 5, size = 3, show.legend = FALSE) +
    geom_point(data = central_tendency, aes(x = ID, y = q50_KL_skeptic), color = "#f8a06d", shape = 5, size = 3, show.legend = FALSE) +
    # scale_y_continuous(trans = pseudo_log_trans(sigma = 1, base = 10), limits = c(0, 0.0075)) +
    labs(x = "Question", y = "Log VOI", color = "Camp") +
    theme_minimal() +
    scale_color_manual(values = palette2) +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      plot.margin = margin(, , , 2, "cm")
    )
}

plotMe(reshapedData)
ggsave("VoI_log.png", width = 10, height = 8)

# Ordered versions of same
reshapedData$ID <- factor(reshapedData$ID, levels = central_tendency$ID[order(central_tendency$q50_KL_concerned)])

plotMe(reshapedData)
ggsave("VoI_log_ordered_concerned.png", width = 10, height = 9)

reshapedData$ID <- factor(reshapedData$ID, levels = central_tendency$ID[order(central_tendency$q50_KL_skeptic)])

plotMe(reshapedData)
ggsave("VoI_log_ordered_skeptic.png", width = 10, height = 9)
```

```{r}
if (FALSE) {
  CG30a <- reshapedData %>%
    filter(ID == "CG30a") %>%
    select(person, PU, PUc, Pc, punotc)

  people <- unique(cg30a$person)
  numpeople <- length(people)

  # create a symmetric adjacency matrix where each cell is the vod between the two people
  cg30a_vod <- matrix(0, nrow = numpeople, ncol = numpeople)
  rownames(cg30a_vod) <- cg30a$person
  colnames(cg30a_vod) <- cg30a$person

  for (i in 1:(numpeople - 1)) {
    for (j in (i + 1):numpeople) {
      cg30a_vod[i, j] <- VoD_log_gmod_easy(cg30a, people[i], people[j])
      cg30a_vod[j, i] <- cg30a_vod[i, j] # use the same value for the 'mirror' cell
    }
  }
}
```

```{r}
# Fiedler ordering & heatmap

# install necessary packages
library(igraph)
library(pheatmap)

make_fiedler_ordered_heatmap <- function(mat, title) {
  #' @param mat a matrix of pairwise VoD for ONE QUESTION

  # NOTE: NOT SURE IF THIS IS THE RIGHT THING TO DO
  mat[is.nan(mat)] <- 0
  mat[is.na(mat)] <- 0

  # create an igraph object
  g <- graph_from_adjacency_matrix(mat, mode = "undirected", weighted = TRUE, diag = FALSE)

  # calculate Laplacian matrix
  laplacian <- laplacian_matrix(g)

  # compute eigenvalues and eigenvectors
  eigen_info <- eigen(laplacian)

  # get the Fiedler vector - the eigenvector of the second smallest eigenvalue
  fiedler_vector <- eigen_info$vectors[, which.min(eigen_info$values[eigen_info$values > min(eigen_info$values)])]

  # create an ordering based on the Fiedler vector
  fiedler_ordering <- order(fiedler_vector)

  # reorder the adjacency matrix
  mat_reordered <- mat[fiedler_ordering, fiedler_ordering]

  # Start png device
  png(paste0("p8_heatmaps/heatmap_anon_", title, ".png"), width = 800, height = 800)

  # create a heatmap
  pheatmap(mat_reordered, main = title, angle_col = "45")

  # Close device
  dev.off()
}
```

```{r}
library(igraph)
library(ggplot2)
library(reshape2)
library(RColorBrewer)

make_fiedler_ordered_heatmap_2 <- function(mat, title) {
  # Get the Fiedler order
  g <- graph_from_adjacency_matrix(mat, mode = "undirected", weighted = TRUE, diag = FALSE)
  laplacian <- laplacian_matrix(g)
  eigen_info <- eigen(laplacian)
  fiedler_vector <- eigen_info$vectors[, which.min(eigen_info$values[eigen_info$values > min(eigen_info$values)])]
  order_rows <- order(fiedler_vector)

  # Convert the matrix to a data frame in long format
  df <- melt(mat)

  # Order the Var1 and Var2 based on the Fiedler ordering
  df$Var1 <- factor(df$Var1, levels = rownames(mat)[order_rows])
  df$Var2 <- factor(df$Var2, levels = rownames(mat)[order_rows])

  # Remove upper triangle
  df <- df[as.integer(df$Var2) <= as.integer(df$Var1), ]

  # Get the default pheatmap colors
  colors_pheatmap <- colorRampPalette(rev(brewer.pal(n = 7, name = "RdYlBu")))(100)

  # Plot using ggplot2
  p <- ggplot(df, aes(x = Var1, y = Var2, fill = value)) +
    geom_tile(color = "white") +
    scale_fill_gradientn(colors = colors_pheatmap) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      plot.margin = margin(, , , 2, "cm"),
      axis.text.y = element_text(angle = 0)
    ) +
    labs(title = title, fill = "VOD") +
    xlab(NULL) +
    ylab(NULL)

  # Save the plot
  ggsave(filename = paste0("p8_heatmaps/lower_triangle_", title, ".png"), plot = p, width = 8, height = 7)
}
```

```{r}
# Select the unique reshapedData IDs
uniqueIDs <- unique(reshapedData$ID)

for (question in uniqueIDs) {
  questionData <- reshapedData %>%
    filter(ID == question) %>%
    select(person, PU, PUc, Pc, punotc)
  people <- unique(questionData$person)
  numpeople <- length(people)

  # create a symmetric adjacency matrix where each cell is the vod between the two people
  vod_matrix <- matrix(0, nrow = numpeople, ncol = numpeople)
  rownames(vod_matrix) <- questionData$person
  colnames(vod_matrix) <- questionData$person

  for (i in 1:(numpeople - 1)) {
    for (j in (i + 1):numpeople) {
      vod_matrix[i, j] <- VoD_log_gmod_easy(questionData, people[i], people[j])
      vod_matrix[j, i] <- vod_matrix[i, j] # use the same value for the 'mirror' cell
    }
  }

  make_fiedler_ordered_heatmap_2(vod_matrix, question)
}
```

```{r}
# PAIRWISE LOG gmod VOD

# For each question (ID) in reshapedData, for each pair of people (person), calculate the VoD (log, gmod)

results <- list()

# Split dataframe by ID
split_data <- split(reshapedData, reshapedData$ID)

for (id in unique(reshapedData$ID)) {
  dt_subset <- split_data[[as.character(id)]]

  # Identify all unique person combinations
  people <- unique(dt_subset$person)
  combinations <- combn(people, 2) # Gets all combinations of people taken 2 at a time

  # For each combination, compute the statistic
  for (col in seq(1, ncol(combinations))) {
    id1 <- combinations[1, col]
    id2 <- combinations[2, col]

    statistic_value_1 <- VoD_log_gmod_easy(dt_subset, id1, id2)
    statistical_value_2 <- maxVOD_easy(dt_subset, id1, id2)

    results[[paste(id, id1, id2, "VoD", sep = "_")]] <- statistic_value_1
    results[[paste(id, id1, id2, "maxVOD", sep = "_")]] <- statistical_value_2
  }
}

# Convert the list of results to a dataframe
results_df <- data.frame(ID_pair_statistic = names(results), value = unlist(results))

# Filter to only the rows that contain (Skeptical) AND (Concerned)
results_df_cross_camp <- results_df %>%
  filter(grepl("Skeptical", ID_pair_statistic) & grepl("Concerned", ID_pair_statistic))

# Extract ID, stat, and value
results_df_cross_camp <- results_df_cross_camp %>%
  mutate(
    ID = gsub("_.+$", "", ID_pair_statistic),
    pair = gsub("^[^_]+_", "", gsub("_[^_]+$", "", ID_pair_statistic)),
    stat = gsub("^.+_", "", ID_pair_statistic)
  ) %>%
  select(-ID_pair_statistic)

# Reshape the data frame to have VoD and maxVOD columns
results_df_cross_camp <- results_df_cross_camp %>%
  pivot_wider(names_from = stat, values_from = value)

# Add percent of max VOD
results_df_cross_camp <- results_df_cross_camp %>%
  mutate(percent_max_VoD = VoD / maxVOD)

results_df_cross_camp_agg <- results_df_cross_camp %>%
  group_by(ID) %>%
  summarize(
    median_VoD = quantile50(VoD),
    median_VoD_pom = quantile50(percent_max_VoD)
  ) %>%
  arrange(desc(median_VoD))

write.csv(results_df_cross_camp_agg, "P8_results_df_cross_camp_VoD_agg.csv", row.names = FALSE)
```

```{r}
options(pillar.sigfig = 5)

results_df_cross_camp_agg %>% filter(grepl("Alignment researchers", ID))
```

```{r}
# PAIRWISE LEVEL VOD

# For each question (ID) in reshapedData, for each pair of people (person), calculate the VoD (log, gmod)

results <- list()

# Split dataframe by ID
split_data <- split(reshapedData, reshapedData$ID)

for (id in unique(reshapedData$ID)) {
  dt_subset <- split_data[[as.character(id)]]

  # Identify all unique person combinations
  people <- unique(dt_subset$person)
  combinations <- combn(people, 2) # Gets all combinations of people taken 2 at a time

  # For each combination, compute the statistic
  for (col in seq(1, ncol(combinations))) {
    id1 <- combinations[1, col]
    id2 <- combinations[2, col]

    statistic_value_1 <- VoD_level_easy(dt_subset, id1, id2)
    statistical_value_2 <- maxVOD_level_easy(dt_subset, id1, id2)

    results[[paste(id, id1, id2, "VoD", sep = "_")]] <- statistic_value_1
    results[[paste(id, id1, id2, "maxVOD", sep = "_")]] <- statistical_value_2
  }
}

# Convert the list of results to a dataframe
results_df <- data.frame(ID_pair_statistic = names(results), value = unlist(results))

# Filter to only the rows that contain (Skeptical) AND (Concerned)
results_df_cross_camp <- results_df %>%
  filter(grepl("Skeptical", ID_pair_statistic) & grepl("Concerned", ID_pair_statistic))

# Extract ID, stat, and value
results_df_cross_camp <- results_df_cross_camp %>%
  mutate(
    ID = gsub("_.+$", "", ID_pair_statistic),
    pair = gsub("^[^_]+_", "", gsub("_[^_]+$", "", ID_pair_statistic)),
    stat = gsub("^.+_", "", ID_pair_statistic)
  ) %>%
  select(-ID_pair_statistic)

# Reshape the data frame to have VoD and maxVOD columns
results_df_cross_camp <- results_df_cross_camp %>%
  pivot_wider(names_from = stat, values_from = value)

# Add percent of max VOD
results_df_cross_camp <- results_df_cross_camp %>%
  mutate(percent_max_VoD = VoD / maxVOD)

results_df_cross_camp_agg <- results_df_cross_camp %>%
  group_by(ID) %>%
  summarize(
    median_VoD = quantile50(VoD, na.rm = TRUE),
    median_VoD_pom = quantile50(percent_max_VoD, na.rm = TRUE)
  ) %>%
  arrange(desc(median_VoD))

write.csv(results_df_cross_camp_agg, "P8_results_df_cross_camp_VoD_LEVEL_agg.csv", row.names = FALSE)
```

```{r}
VoD_quantiles <- results_df_cross_camp %>%
  group_by(ID) %>%
  summarize(
    median = quantile50(VoD),
    # Give me two more columns: 25th quantile and 75th quantile
    quantile_25 = quantile(VoD, 0.25, na.rm = TRUE),
    quantile_75 = quantile(VoD, 0.75, na.rm = TRUE)
  ) %>%
  mutate(
    IQR = quantile_75 - quantile_25,
    platform = grepl("Platform", ID)
  ) %>%
  arrange(desc(median)) %>%
  select(ID, median, quantile_25, quantile_75, IQR, platform)

VoD_quantiles <- merge(VoD_quantiles, VoD_between_medians, by = "ID")

# Make ID a factor, ordered by median
VoD_quantiles$ID <- factor(VoD_quantiles$ID, levels = VoD_quantiles$ID[order(VoD_quantiles$median)])

# Plot VoD_quantiles with x axis is ID (question) and plot box with quantile_25 and quantile_75
ggplot(VoD_quantiles, aes(x = ID, y = median)) +
  geom_boxplot() +
  geom_errorbar(aes(ymin = quantile_25, ymax = quantile_75, color = platform), width = 0.2) +
  labs(x = "Question ID", y = "Log VoD", title = "Log VoD Between Cross-Camp Pairs") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) #+
# Plot VoD_log_gmod as green stars
# geom_point(aes(y = VoD_log_gmod, color = 'green'), shape = 8, size = 3, show.legend = FALSE)

ggsave("VoD_log_gmod_easy.png", width = 8, height = 8)
```

```{r}
# Plot VoD_quantiles$median for each question as bar graph - over/under zero
bars <- ggplot(VoD_quantiles, aes(x = ID, y = median, fill = median)) +
  geom_bar(stat = "identity") +
  labs(x = "Question", y = "Median Log VoD For Cross-Camp Pairs") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.margin = margin(, , , 2, "cm"),
    axis.title.x = element_text(margin = margin(t = 20)),
    legend.position = "none"
  ) +
  scale_fill_gradient2(
    low = "orange", high = "green", midpoint = 0,
    limits = c(min(VoD_quantiles$median), max(VoD_quantiles$median))
  )

ggsave("VoD_log_gmod_easy_bars.png", width = 9, height = 8)
```

```{r}
here <- results_df_cross_camp %>%
  separate(ID_pair, c("ID", "person1", "person2"), sep = "_")

order <- here %>%
  group_by(ID) %>%
  summarize(median = quantile50(value)) %>%
  arrange(median) %>%
  pull(ID)

here <- merge(here, VoD_quantiles, by = "ID")

jazz <- here %>%
  ggplot(aes(x = factor(ID, levels = order), y = value)) +
  geom_boxplot(aes(fill = factor(ID)), alpha = 0.7, outlier.shape = NA) + # fill boxes with color
  geom_jitter(aes(color = factor(ID)), width = 0.2, alpha = 0.5) + # jittered points with color
  stat_summary(fun = quantile50, geom = "point", shape = 23, size = 3, color = "black") +
  labs(x = "Question", y = "VoD (cross-camp pairs)", fill = "Question", color = "Question") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.margin = margin(, , , 2, "cm"),
    legend.position = "none"
  )

jazz <- here %>%
  ggplot(aes(x = factor(ID, levels = order), y = value)) +
  geom_boxplot(aes(fill = median), alpha = 0.7, outlier.shape = NA) +
  geom_jitter(aes(color = median), width = 0.2, alpha = 0.5) +
  stat_summary(fun = quantile50, geom = "point", shape = 23, size = 3, color = "black") +
  labs(x = "Question", y = "VoD (cross-camp pairs)", fill = "Median", color = "Median") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "grey", midpoint = 0) +
  scale_color_gradient2(low = "blue", high = "red", mid = "grey", midpoint = 0) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.margin = margin(, , , 2, "cm"),
    legend.position = "none"
  )

ggsave("P8_VOD_boxplot.png", jazz, width = 10, height = 8)
```

```{r}
# Load necessary libraries
library(ggplot2)
library(ggridges)

ordering <- VoD_quantiles %>%
  arrange(median) %>%
  pull(ID)

results_df_cross_camp$ID <- factor(results_df_cross_camp$ID, levels = ordering)

# Create dot/half violin plots
p <- ggplot(results_df_cross_camp, aes(x = ID, y = value)) +
  geom_violin(aes(fill = ID), scale = "width") +
  # geom_dotplot(aes(fill = ID), binaxis = 'y', stackdir = 'center', position = position_nudge(x = -0.2), dotsize = 0.5) +
  theme_minimal() +
  labs(title = "Dot and Half-Violin Plots by ID", y = "Value", x = "ID") +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.margin = margin(, , , 2, "cm"),
    legend.position = "none"
  )

ggsave("VoD_log_violinish.png", width = 9, height = 8)
```

```{r}
results_df_cross_camp_f <- results_df_cross_camp %>%
  select(ID, value) %>%
  group_by(ID) %>%
  summarize(median_cross_camp_pairs = quantile(value, type = 1, p = 0.5, na.rm = TRUE))

median_forecasters_f <- arjj %>%
  select(ID, VoD_log_gmod)

compare_VoD <- merge(results_df_cross_camp_f, median_forecasters_f, by = "ID", all.x = TRUE) %>%
  arrange(VoD_log_gmod) %>%
  rename(VoD_between_medians = VoD_log_gmod) %>%
  distinct()

write.csv(compare_VoD, "compare_VoD.csv", row.names = FALSE)
```

```{r}
# Get component-wise VOI for each question
compwise <- reshapedData %>%
  group_by(ID, Camp) %>%
  summarize(median(PU), median(PUc), median(Pc), median(punotc), median(VoI_log, na.rm = TRUE))

cc <- as.data.frame(compwise) %>%
  group_by(ID, Camp) %>%
  mutate(
    componentWiseVoI = VoI_log(`median(PU)`, `median(PUc)`, `median(Pc)`, `median(punotc)`),
    PUc_x_Pc_less_than_PU = `median(PUc)` * `median(Pc)` < `median(PU)`
  ) %>%
  mutate(punotc_check = punotc(`median(Pc)`, `median(PUc)`, `median(PU)`))

write.csv(cc, "componentwise.csv", row.names = FALSE)
```

```{r}
# Group by question, get gmod Log VOI for each question
aggVoi <- reshapedData %>%
  group_by(ID) %>%
  summarise(
    gmod_voi = exp(mean(log(VoI_log), na.rm = TRUE)),
    mean_voi = mean(VoI_log, na.rm = TRUE),
    median_voi = median(VoI_log, na.rm = TRUE)
  )
```

```{r}
# Order ID levels
print(aggVoi %>% arrange(median_voi), n = 40)
```

```{r}
# Is it ever the case that PU is zero but PUc is not? That would be inconsistent
reshapedData %>%
  filter(PU == 0 & PUc != 0)
```

```{r}
# Histogram of median Log VOI's
ggplot(aggVoi, aes(x = median_voi)) +
  geom_histogram(binwidth = 0.00001) +
  labs(x = "Median Log VOI", y = "Count")

ggsave("median_voi_hist.png", width = 10, height = 10)
```

```{r}
# Histogram of mean Log VOI's
ggplot(aggVoi, aes(x = mean_voi)) +
  geom_histogram(binwidth = 0.001) +
  labs(x = "Mean Log VOI", y = "Count")

ggsave("mean_voi_hist.png", width = 10, height = 10)
```

```{r}
# Create a column in reshapedData that tells me whether >0.67 of the people updated in the same direction, and if so, which
directions <- reshapedData %>%
  group_by(ID, Camp) %>%
  summarize(numUp = sum(direction == "up"), numDown = sum(direction == "down"), numNoChange = sum(direction == "no change"), total = n()) %>%
  # Determine ***REMOVED*** direction
  mutate(
    ***REMOVED***Direction =
      ifelse(numUp > 0.67 * total, "up",
        ifelse(numDown > 0.67 * total, "down", "no ***REMOVED***")
      )
  )

M <- reshapedData %>%
  group_by(ID, Camp) %>%
  summarize(median_KL = median(VoI_log, na.rm = TRUE))

directions2 <- merge(M, directions, by = c("ID", "Camp")) %>%
  arrange(desc(median_KL))
write.csv(directions2, "directions2.csv", row.names = FALSE)

print(directions %>% select(ID, Camp, ***REMOVED***Direction), n = 60)

write.csv(directions %>% select(ID, Camp, numUp, numDown, numNoChange, total), "directions.csv", row.names = FALSE)
```

```{r}
library(data.table)
library(reshape2)

prepost <- data.table(
  # Make a dataframe with four columns: Camp, Initial forecast, Final forecast, and Reason for update. Camp is "Concerned" 11 times and "Skeptical" 11 times
  Camp = c(rep("Concerned", 11), rep("Skeptical", 11)),
  ID = 1:22,
  Initial = c(65, 60, 35, 30, 30, 25, 22.9, 21, 10, 9, 4, 3, 1.5, .5, .4, .2, .1, .1, .1, .05, .000001, 0.0000001),
  Final = c(55, 30, 35, 30, 30, 20, 17.5, 18, 10, 9, 2.4, 2, 1, .5, 1.1, .1, .12, .2, .02, .07, .0001, .0001)
)

# reshape prepost so that Initial and Final are in one column, and which is which is denoted by a new column InitFinal
prepostMelt <- melt(prepost, id.vars = c("Camp", "ID"), variable.name = "InitFinal", value.name = "Value")
```

```{r}
dogData <- prepostMelt %>% filter(!is.na(Value))
```

```{r}
library(ggrepel)

# Calculate medians for each group
medians <- dogData %>%
  group_by(Camp, InitFinal) %>%
  summarise(Median = quantile(Value, 0.5, type = 1))

# Create the ggplot
dog <- ggplot(dogData, aes(x = InitFinal, y = Value, color = Camp)) +
  theme_minimal() +
  scale_color_manual(values = palette2) +
  # Box plot without whiskers
  geom_boxplot(outlier.shape = NA, coef = 0) +
  # Add individual points
  geom_point(position = position_jitterdodge(jitter.width = 0.05), alpha = 0.5) +
  # Log scale
  scale_y_log10(limits = c(1e-7, 66)) +
  # Add labels to the points
  geom_text_repel(
    aes(label = Value),
    box.padding = 0.5,
    point.padding = 0.5,
    size = 3,
    position = position_jitterdodge(jitter.width = 0.05),
    max.overlaps = 4
  ) +
  geom_label(
    data = medians,
    fill = "white",
    aes(y = Median, label = round(Median, 2), color = Camp, fill = "white"),
    position = position_dodge(0.75),
    size = 3,
    label.size = 0.5,
    show.legend = FALSE # This prevents legend for the fill aesthetic
  ) +
  # Add labels and themes as you like
  labs(x = "", y = "P(AI Extinction by 2100)", color = "Camp")

ggsave("init-final-box-and-dots-p8.png", dog, width = 7, height = 7)
```

```{r}
VoI_log(0.01, 0.0001, 0.08)
VoI_log(0.01, 0.1, 0.08)
```

```{r}
VoI_log(0.01, 0.001, 0.08)
VoI_log(0.01, 0.0001, 0.08)
```

```{r}
ss <- "(C2*log(C2/D2)+(1-C2)*log((1-C2)/(1-D2)))*B2 + (E2*log(E2/D2)+(1-E2)*log((1-E2)/(1-D2)))*(1-B2)"

# In that string, replace C2 with PUc, D2 with PU, E2 with punotc, and B2 with Pc
ss <- gsub("C2", "PUc", ss)
ss <- gsub("D2", "PU", ss)
ss <- gsub("E2", "punotc", ss)
ss <- gsub("B2", "Pc", ss)

CQ30 <- VoI_log(1e-8, 1e-7, 1e-6)
JC50 <- VoI_log(1e-8, 4e-8, 1e-6)
LR70 <- VoI_log(1e-8, 1e-6, 1e-5)
CQ40 <- VoI_log(1e-8, 1e-7, 1e-8)
HN50 <- VoI_log(1e-8, 1e-8, 1e-1)
```

```{r}
# This is just a little something about aggregation for research discussion slide deck
subby <- reshapedData %>% filter(grepl("Platform: AI", ID))

longData <- tidyr::pivot_longer(subby, c("PU", "Pc", "PUc"))

# Plotting
ggplot(longData, aes(x = name, y = value, color = Camp)) +
  geom_point(position = position_jitterdodge(jitter.width = 0.2, dodge.width = 0.3)) +
  labs(x = "Forecasts", y = "Value") +
  theme_minimal()

ggsave("componentwise_AI_regulation.png", width = 10, height = 8)
```

```{r}
# Ezra's "best of" idea: When we know the answers to all these cruxes in 2030, where are we?

# For each PERSON, what's the highest VOI question
best_of_x <- reshapedData %>%
  group_by(person, Camp) %>%
  filter(percent_max_VoI == max(percent_max_VoI, na.rm = TRUE)) %>%
  select(person, Camp, percent_max_VoI, ID)

n_per_question_x <- best_of_x %>%
  group_by(ID) %>%
  summarize(n = n()) %>%
  arrange(desc(n))

# And if we remove Transformative growth?
best_of <- reshapedData %>%
  filter(grepl("Transformative", ID) == FALSE) %>%
  group_by(person, Camp) %>%
  filter(percent_max_VoI == max(percent_max_VoI, na.rm = TRUE)) %>%
  select(person, Camp, percent_max_VoI, ID)

n_per_question_skeptics <- best_of %>%
  filter(Camp == "Skeptical") %>%
  group_by(ID) %>%
  summarize(n = n()) %>%
  arrange(desc(n))

n_per_question_concerned <- best_of %>%
  filter(Camp == "Concerned") %>%
  group_by(ID) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
```

```{r}
# Headline stat: Median of the highest % of max VoI for each camp
best_of %>%
  group_by(Camp) %>%
  summarize(
    median = quantile50(percent_max_VoI),
    mean = mean(percent_max_VoI),
    # sd = sd(percent_max_VoI),
    min = min(percent_max_VoI),
    max = max(percent_max_VoI),
    p05 = quantile(percent_max_VoI, 0.05),
    p95 = quantile(percent_max_VoI, 0.95)
  )
```

```{r}
# Kinda surprised the skeptic median for that stat turned out larger since the concerned % of max VoIs generally seemed higher
reshapedData %>%
  group_by(Camp) %>%
  summarize(
    median = quantile50(percent_max_VoI),
    mean = mean(percent_max_VoI),
    sd = sd(percent_max_VoI),
    min = min(percent_max_VoI),
    max = max(percent_max_VoI)
  )
```

```{r}
# Forget maxVOD for now
results_df <- results_df %>%
  filter(!grepl("maxVOD", ID_pair_statistic)) %>%
  # Trim "_VoD" from ID_pair_statistic
  mutate(ID_pair = gsub("_VoD", "", ID_pair_statistic))

# Get the "best of" VoD: for each pair, what's the question that yields highest VoD?
aa <- results_df %>%
  separate(ID_pair, into = c("C", "ID1", "ID2"), sep = "_") %>%
  mutate(pair = paste0(pmin(ID1, ID2), "_", pmax(ID1, ID2))) %>%
  filter(!grepl("Transformative", C)) %>%
  group_by(pair) %>%
  filter(value == max(value, na.rm = TRUE)) %>%
  select(C, pair, value) %>%
  separate(pair, into = c("ID1", "ID2"), sep = "_")

people <- unique(reshapedData$person)
numpeople <- length(people)

# create a symmetric adjacency matrix where each cell is the vod between the two people
vod_matrix <- matrix(0, nrow = numpeople, ncol = numpeople)
rownames(vod_matrix) <- people
colnames(vod_matrix) <- people

for (i in 1:(numpeople - 1)) {
  for (j in (i + 1):numpeople) {
    a <- aa %>%
      filter(ID1 == people[i], ID2 == people[j]) %>%
      pull(value)
    b <- aa %>%
      filter(ID2 == people[i], ID1 == people[j]) %>%
      pull(value)
    if (length(a) == 0) {
      vod_matrix[i, j] <- b[1]
      vod_matrix[j, i] <- b[1] # use the same value for the "mirror" cell
    } else {
      vod_matrix[i, j] <- a[1]
      vod_matrix[j, i] <- a[1]
    }
  }
}

make_fiedler_ordered_heatmap(vod_matrix, "Best Of VoD for Each Pair (without Transformative Growth)")
```

```{r}
# Join ID1 and ID2 into one column & filter to cross-camp pairs
bb <- aa %>%
  mutate(ID = paste0(pmin(ID1, ID2), "_", pmax(ID1, ID2))) %>%
  select(C, ID, value) %>%
  filter(grepl("Concerned", ID) & grepl("Skeptical", ID))

bestVOD <- bb %>%
  group_by(C) %>%
  summarize(n = n()) %>%
  arrange(desc(n))

write.csv(bestVOD, "number of cc pairs for whom ID was most convergent crux.csv", row.names = FALSE)
```

```{r}
reshapedData %>%
  filter(grepl("James", person) | grepl("Xander", person), grepl("Evidence", ID)) %>%
  select(Camp, PU, Pc, PUc, ID, person, punotc, percent_max_VoI)
```

```{r}
# How many times does each question come up in aa$value
aa %>%
  group_by(C) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) %>%
  print(n = 31)
```

```{r}
stacked <- reshapedData %>%
  group_by(ID, Camp) %>%
  summarize(
    percent_up = sum(direction == "up") / n() * 100,
    percent_down = sum(direction == "down") / n() * 100,
    percent_no_change = sum(direction == "no change") / n() * 100
  )
```
